---
title: "time series"
output: html_notebook
---
Forecasting the fluctuation patterns of stock prices has been an extensively researched domain both in statistics and data science literature. Although the efficient market hypothesis asserts that it is impossible to predict stock prices, several cases established that if correctly formulated and modeled, prediction of stock prices can be done with a fairly high level of accuracy. This latter viewpoint focused on the construction of robust machine learning and statistical models based on the careful choice of variables/hyper parameters and appropriate functional forms or models of forecasting. Along the same lines a strand of research in the literature focused on time series analysis and decomposition for forecasting future values of stocks.

The literature attempting to prove or disprove the efficient market hypothesis can be classified into three strands, according to the choice of variables and techniques of estimation and forecasting. The first strand consists of studies using simple regression techniques on cross-sectional data. The second strand of the literature has used time series models and techniques to forecast stock returns following economic tools like autoregressive integrated moving average (ARIMA), Granger causality test, autoregressive distributed lag (ARDL) and quantile regression (QR) to forecast stock prices. The third strand includes work using machine learning tools for the prediction of stock returns.

Time-series forecasting models are the models that are capable to predict future values based on previously observed values.  A time series is simply a series of data points ordered in time. In a time series, time is often the independent variable and the goal is usually to make a forecast for the future.
However, there are other aspects that come into play when dealing with time series.
Is it stationary?
Is there a seasonality?
Is the target variable autocorrelated?

Autocorrelation
Informally, autocorrelation is the similarity between observations as a function of the time lag between them.
Above is an example of an autocorrelation plot. Looking closely, you realize that the first value and the 24th value have a high autocorrelation. Similarly, the 12th and 36th observations are highly correlated. This means that we will find a very similar value at every 24 unit of time.
Notice how the plot looks like sinusoidal function. This is a hint for seasonality, and you can find its value by finding the period in the plot above, which would give 24h.

Seasonality
Seasonality refers to periodic fluctuations. For example, electricity consumption is high during the day and low during night, or online sales increase during Christmas before slowing down again.
As you can see above, there is a clear daily seasonality. Every day, you see a peak towards the evening, and the lowest points are the beginning and the end of each day.
Remember that seasonality can also be derived from an autocorrelation plot if it has a sinusoidal shape. Simply look at the period, and it gives the length of the season.


Stationarity
Stationarity is an important characteristic of time series. A time series is said to be stationary if its statistical properties do not change over time. In other words, it has constant mean and variance, and covariance is independent of time.
Looking again at the same plot, we see that the process above is stationary. The mean and variance do not vary over time.



The analyzed index used here will be BTC, the conversion  rate of bitcoin to USD, with data extracted from Yahoo Finance using the package quantmod in R.
```{r}
library(PerformanceAnalytics)
library(quantmod)
library(tidyverse)
library(modeldata)
library(forecast)
library(tidymodels)
library(modeltime)
library(timetk)
library(lubridate)
```


```{r}
BTC <- getSymbols("BTC-USD", src = "yahoo", from = "2013-01-01", to = "2020-11-01", auto.assign = FALSE)

Op(BTC)
Cl(BTC)
Hi(BTC)
Lo(BTC)
Vo(BTC)
```
A possibility given by quantmod is the calculation of returns for different periods. For example, itâ€™s possible to calculate the returns by day, week, month, quarter and year, just by using the following commands:

```{r}
plot(dailyReturn(BTC))
plot(weeklyReturn(BTC))
```
Here we add the date column to the time-series data set.
```{r}
ts=as.data.frame(dailyReturn(BTC))
ts$date=as.Date (row.names(ts)) 
```

and split the data to trining and test fro cross validation.
```{r}
train_data <- training(initial_time_split(ts, prop = .8))
test_data <- testing(initial_time_split(ts, prop = .8))

```
Here is the visualization of propre training test split for time series data.
```{r}
train_data %>% mutate(type = "train") %>% 
  bind_rows(test_data %>% mutate(type = "test")) %>% 
  ggplot(aes(x = date, y =daily.returns, color = type)) + 
  geom_line()
```

The model fitting procedure is similar to that tidy models, here is the example for an ARIMA model fitted on the training data,
```{r}
arima_model <- arima_reg() %>% 
  set_engine("auto_arima") %>% 
  fit(daily.returns~date, data = train_data)
```


a prophet regression model fitted on the training data,
```{r}
prophet_model <- prophet_reg() %>% 
  set_engine("prophet") %>% 
  fit(daily.returns~date, data = train_data)
```

and a linear regression model fitted on the training data.
```{r}
tslm_model <- linear_reg() %>% 
  set_engine("lm") %>% 
  fit(daily.returns~as.numeric(date) + factor(month(date, label = TRUE)), data = train_data)

```

In order to put all the results together and compare them you have to creat a tibble of the model out comes.
```{r}
forecast_table <- modeltime_table(
  arima_model,
  prophet_model,
  tslm_model
)
```


Then to compare the performances the models should be fitted on the test data.
```{r}
forecast_table %>% 
  modeltime_calibrate(test_data) %>% 
  modeltime_accuracy()
```


it is also possible to plot the comparative performance .
```{r}
forecast_table %>% 
  modeltime_calibrate(test_data) %>% 
  modeltime_forecast(actual_data = test_data) %>% 
  plot_modeltime_forecast()
```

